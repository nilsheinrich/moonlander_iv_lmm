---
title: "boxcox_analysis"
author: "Nils Wendel Heinrich"
date: "2025-02-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, include=FALSE}
library(tidyverse)
library(arrow)
library(MASS)
library(lme4)
library(lmerTest)

set.seed(36)
N_iterations <- 10000

```

```{r data, include=FALSE}

setwd('/Users/heinrich/Projects/Moonlander_iv_LMM/')

soc_data <- read_csv('data/soc_data_iv.csv')

```

# Performance
```{r performance, include=FALSE}

# performance within participant
performance_by_ID <- soc_data %>%
  group_by(ID) %>%
  summarize(performance = mean(done))

# performance across participants
mean_overall_performance <- mean(performance_by_ID$performance)
sd_overall_performance <- sd(performance_by_ID$performance)

# identify levels with high failure rate
failure_by_level <- soc_data %>%
  group_by(level) %>%
  summarize(failure_rate = mean(done == 0), total_trials = n()) %>%
  arrange(desc(failure_rate))

# Display results
performance_by_ID
mean_overall_performance
sd_overall_performance
failure_by_level

```


# Predicting SoC

## Box Cox - analysis

```{r box_cox, include=FALSE}

lambda_soc <- boxcox(lm(soc_data$SoC ~ 1))

lambda_soc$x[which(lambda_soc$y == max(lambda_soc$y))]

```

lambda, the expected value is close to 0.5. We will therefore apply a square root transformation to the predicted variable.

## ICC for randon intercept effect ID

```{r null_model, include=FALSE}

null.1 <- lmer(sqrt(SoC) ~ 1 + (1|ID), data = soc_data, REML=FALSE)
summary(null.1)

```

Inter-Class Correlation

$\sqrt{\frac{Variance_{ID}}{Variance_{ID} + Variance_{residual}}}$

```{r ICC, include=FALSE}

0.08894 / (0.08894+0.10419)

```

Roughly 46.05% of the total variance is explained by the random intercept effect ID. 

## Exploring fixed effects
We gathered a multitude of interesting variables that might influence the SoC reported by participants after every trial. In an initial model, we will simply throw all of the in the fixed effects structure and see what sticks.

```{r fix_1, include=FALSE}

fix.1 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + trials_since_last_crash + N_consecutive_crash_success + 
                Neuroticism + Extraversion + Openness + Conscientiousness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.1)

```

**None** of the NEO-FFI variables significantly influence the predicted variable SoC. We might only want to look out for individual dimensions. When deleting some fixed effects there might be more variance loading on the others, revealing a significant effect. Let's see...

For *trials_since_last_crash* we will put in a variable that assesses whether there was a crash in the last trial. This is less informative due to only being binary, but we may reduce complexity this way.

```{r fix_2, include=FALSE}

fix.2 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                Neuroticism + Extraversion + Openness + Conscientiousness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.2)

```

We will keep *crashed_in_last_trial* instead of *trials_since_last_crash*. It's estimate is higher while also reaching significance.

Now we can go on and delete redundant variables, starting with *Conscientiousness* (the one with the worst p value).

```{r fix_3, include=FALSE}

fix.3 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                Neuroticism + Extraversion + Openness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.3)

```

Like we expected, all the other dimensions absorbed the variance of the deleted *Conscientiousness*. The NEO-FFI dimensions seem to be highly correlated in their effect on SoC.

```{r 2vs3, include=FALSE}

anova(fix.2, fix.3)

```
We can delete *Conscientiousness*. It will increase the predictive power of our model (referring to BIC, smaller is better here) while the model likelihood not being significantly different.

Let's try to reduce the model structure even more: deleting *Agreeableness*.
```{r fix_4, include=FALSE}

fix.4 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                Neuroticism + Extraversion + Openness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.4)

anova(fix.3, fix.4)

```
Again a safe deletion without significantly affecting model likelihood but increasing predictive power.

Proceeding by deleting *Extraversion*:
```{r fix_5, include=FALSE}

fix.5 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                Neuroticism + Openness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.5)

anova(fix.4, fix.5)

```
We will proceed with fix.5.

Deleting *Neuroticism*:
```{r fix_6, include=FALSE}

fix.6 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + crashed_in_last_trial + N_consecutive_crash_success + Openness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.6)

anova(fix.5, fix.6)

```
Proceeding with fix.6.

Let's see whether we can safely delete *Openness* as well:
```{r fix_7, include=FALSE}

fix.7 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + crashed_in_last_trial + N_consecutive_crash_success + (1|ID), data = soc_data, REML=FALSE)
summary(fix.7)

anova(fix.6, fix.7)

```
Proceeding with fix.7

Now we can actually try to simplify the model fixed effects structure even more. This will mean that we delete a significant effect, but we will try anyway:

Deleting *crashed_in_last_trial*:
```{r fix_8, include=FALSE}

fix.8 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + N_consecutive_crash_success + (1|ID), data = soc_data, REML=FALSE)
summary(fix.8)

anova(fix.7, fix.8)

```
Here we see a significant difference in the likelihood of the model (referring to Pr(>Chisq)), but the BIC decreases telling us to throw out *crashed_in_last_trial*, as we're going for a model with a reasonable amount of parameters. We will thus proceed with fix.8.

Deleting *N_consecutive_crash_success*:
```{r fix_9, include=FALSE}

fix.9 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + (1|ID), data = soc_data, REML=FALSE)
summary(fix.9)

anova(fix.8, fix.9)

```
Again, by deleting *N_consecutive_crash_success* we see a significant difference in the model likelihood, but the BIC decreases. Proceeding with fix.9.

Deleting *N_drift* (it has the smallest slope)
```{r fix_10, include=FALSE}

fix.10 <- lmer(sqrt(SoC) ~ expectancy + crashed + (1|ID), data = soc_data, REML=FALSE)
summary(fix.10)

anova(fix.9, fix.10)

```
Nope, we see a stark increase in BIC. We will leave it like that and settle on fix.9!

## Exploring random effects structure
Now we're going to open up the slopes of our effects left in the model to change from one ID to the next.

We're using the same approach as before: we will simply dump all possible random slope effects into the random effects structure of the model and see what sticks.

```{r random_1, include=FALSE}

random.1 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + 
                   (1 + expectancy + N_drift + crashed |ID), data = soc_data, REML=FALSE)
summary(random.1)

```
*fixed-effect model matrix is rank deficient so dropping 1 column / coefficient* is warning that we can ignore for now.
*boundary (singular) fit: see help('isSingular')* and *Warning: Model failed to converge...* however we have to take seriously. They tell us that our model is **overparameterized** (we have too many random slopes in there that do not vary across IDs). We have to kick out random slopes.

Starting with *N_drift*:
```{r random_2, include=FALSE}

random.2 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + 
                   (1 + expectancy + crashed |ID), data = soc_data, REML=FALSE)
summary(random.2)

```
The model converges. This will be our starting point.

Let's also drop *crashed*: 
```{r random_3, include=FALSE}

random.3 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed + 
                   (1 + expectancy |ID), data = soc_data, REML=FALSE)
summary(random.3)

```

Deleting expectancy instead:
```{r random_4, include=FALSE}

random.4 <- lmer(sqrt(SoC) ~ expectancy + N_drift + crashed +
                   (1 + crashed |ID), data = soc_data, REML=FALSE)

```
Ok the model converges.

Now we've got a few models to compare:
```{r model_comparison1, echo=FALSE}

anova(fix.9, random.4, random.3, random.2)

```
random.2, the model with the most complex random slope effects structure that converges has the lowest BIC, meaning that it's our "best" model.

```{r taking_a_look, echo=FALSE}

summary(random.2)

```

## Generating simulations based on the final selected model

parametric bootstrap:
```{r bootstrap1, include=FALSE}

confint(random.2, nsim=N_iterations, parm=c('expectancy', 'N_drift', 'crashed'), method='boot')

```
We got a few failed convergences here, but that is ok, we chose the most complex converging model after all.

Results:
                 2.5 %      97.5 %
expectancy  0.06760148  0.12097717
N_drift     0.06381395  0.07749235
crashed    -0.38838600 -0.24049821


# Predicting Expectancy
We hypothesize that the variable *expectancy* reflects the prior belief of participants about the self-efficiency. We will explore what constitutes this prior, i.e. which variables best explain *expectancy*. We already have a candidates in mind...

## Box Cox - analysis
```{r box_cox2, include=FALSE}

lambda_expect <- boxcox(lm(soc_data$expectancy ~ 1))

lambda_expect$x[which(lambda_expect$y == max(lambda_expect$y))]

```

lambda, the expected value is close to 1.0. In this case we won't transform the predicted variable.

## ICC for randon intercept effect ID

```{r null_model_expect, include=FALSE}

null.2 <- lmer(expectancy ~ 1 + (1|ID), data = soc_data, REML=FALSE)
summary(null.2)

```

Inter-Class Correlation
```{r ICC2, include=FALSE}

1.1735 / (1.1735+0.8173)

```

ID explains roughly 58.90% of the total variance in expectancy.

## Exploring fixed effects
Same procedure as above...

```{r fix_1, include=FALSE}

fix.1 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Neuroticism + Extraversion + Openness + Conscientiousness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.1)

```

We see that the NEO-FFI dimensions do not significantly influence expectancy. Only *Openness* "approaches" significance. We will try do delete the other dimensions first. After that we might drop *crashed_in_last_trial*.

Starting with dropping *Conscientiousness*
```{r fix_2, include=FALSE}

fix.2 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Neuroticism + Extraversion + Openness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)

```

```{r 1vs2, include=FALSE}

anova(fix.1, fix.2)

```

Proceeding with fix.2

Let's see if we can reduce complexity even more. Dropping *Agreeableness*:
```{r fix_3, include=FALSE}

fix.3 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Neuroticism + Extraversion + Openness + (1|ID), data = soc_data, REML=FALSE)

anova(fix.2, fix.3)

```
Again deleting the effect is advised. Proceeding with fix.3.

Dropping *Neuroticism*
```{r fix_4, include=FALSE}

fix.4 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Extraversion + Openness + (1|ID), data = soc_data, REML=FALSE)

anova(fix.3, fix.4)

```
Dropping *Neuroticism* also decreases BIC and doesn't significantly affects model likelihood. Proceeding with fix.4.

What about *Extraversion*?
```{r fix_5, include=FALSE}

fix.5 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Openness + (1|ID), data = soc_data, REML=FALSE)

anova(fix.4, fix.5)

```
Proceeding with fix.5.

Can we also drop *Openness*, the last NEO-FFI dimension?
```{r fix_6, include=FALSE}

fix.6 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial 
              + (1|ID), data = soc_data, REML=FALSE)

anova(fix.5, fix.6)

```
We can safely throw out all the NEO-FFI dimensions. 

Now we will attempt to drop *crashed_in_last_trial*. We strongly assumed that this will definitely affect expectancy, but it didn't show significance earlier, so let's see:
```{r fix_7, include=FALSE}

fix.7 <- lmer(expectancy ~ N_consecutive_crash_success + SoC_last_trial 
              + (1|ID), data = soc_data, REML=FALSE)
#summary(fix.7)

anova(fix.6, fix.7)

```
*crashed_in_last_trial* will also be dropped. Proceeding with fix.7.

I don't think it is advised but just try to delete *N_consecutive_crash_success* (it has the smaller slope)
```{r fix_8, include=FALSE}

fix.8 <- lmer(expectancy ~ SoC_last_trial 
              + (1|ID), data = soc_data, REML=FALSE)

anova(fix.7, fix.8)

```
Here, we see again an increase in BIC. We will keep the model structure defined in fix.7 and proceed from there.

## Exploring random effects structure

We only have 2 fixed effects left. The model is not complex, but now we will introduce random slope effects into the model and see whether either fixed effect changes across IDs.

```{r random_1, include=FALSE}

random.1 <- lmer(expectancy ~   N_consecutive_crash_success + SoC_last_trial 
                 + (1 + N_consecutive_crash_success + SoC_last_trial |ID), data = soc_data, REML=FALSE)
summary(random.1)

```

Model failed to converge... We need to reduce the complexity of the random effects structure. 

Deleting *N_consecutive_crash_success* (arbitrarily chosen starting point):
```{r random_2, include=FALSE}

random.2 <- lmer(expectancy ~ N_consecutive_crash_success + SoC_last_trial 
                 + (1 + SoC_last_trial |ID), data = soc_data, REML=FALSE)
#summary(random.2)

```
Ok, this model converges. We will take it in the model selection.

Deleting *SoC_last_trial* instead:
```{r random_3, include=FALSE}

random.3 <- lmer(expectancy ~ N_consecutive_crash_success + SoC_last_trial 
                 + (1 + N_consecutive_crash_success |ID), data = soc_data, REML=FALSE)
#summary(random.3)

```

Let's compare the converged models
```{r model_comparison2, include=FALSE}

anova(fix.7, random.2, random.3)

```
random.3 the model with the random slope effect of *N_consecutive_crash_success* yields the lowest BIC. We will use random.3 for hypothesis testing.


```{r final_selected_model2, include=FALSE}

summary(random.3)

```
That is interesting. We see that the random slope effect in *N_consecutive_crash_success* caused the fixed effect to disappear. That can happen and it actually tells us that the difference in the slopes of this effect is far too different from one person to the next to be a significant robust effect.

parametric bootstrap:
```{r bootstrap2, include=FALSE}

confint(random.3, nsim=N_iterations, parm=c('N_consecutive_crash_success', 'SoC_last_trial'), method='boot')

```
...ignoring the non-convergence warnings...

Results:
                                   2.5 %     97.5 %
N_consecutive_crash_success -0.002845176 0.01946382
SoC_last_trial               0.326064655 0.36064143


