---
title: "boxcox_analysis"
author: "Nils Wendel Heinrich"
date: "2025-02-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, include=FALSE}
library(tidyverse)
library(arrow)
library(MASS)
library(lme4)
library(lmerTest)

set.seed(36)
N_iterations <- 10000

```

```{r data, include=FALSE}

setwd('/Users/heinrich/Projects/Moonlander_iv_LMM/')

soc_data <- read_csv('data/soc_data_iv.csv')

```

# Performance
```{r performance, include=FALSE}

# performance within participant
performance_by_ID <- soc_data %>%
  group_by(ID) %>%
  summarize(performance = mean(done))

# performance across participants
mean_overall_performance <- mean(performance_by_ID$performance)
sd_overall_performance <- sd(performance_by_ID$performance)

# identify levels with high failure rate
failure_by_level <- soc_data %>%
  group_by(level) %>%
  summarize(failure_rate = mean(done == 0), total_trials = n()) %>%
  arrange(desc(failure_rate))

# Display results
performance_by_ID
mean_overall_performance
sd_overall_performance
failure_by_level

```


# Predicting SoC

## Box Cox - analysis

```{r box_cox, include=FALSE}

lambda_soc <- boxcox(lm(soc_data$SoC ~ 1))

lambda_soc$x[which(lambda_soc$y == max(lambda_soc$y))]

```

lambda, the expected value is close to 0.5. We will therefore apply a square root transformation to the predicted variable.

## ICC for randon intercept effect ID

```{r null_model, include=FALSE}

null.1 <- lmer(sqrt(SoC) ~ 1 + (1|ID), data = soc_data, REML=FALSE)
summary(null.1)

```

Inter-Class Correlation

$\sqrt{\frac{Variance_{ID}}{Variance_{ID} + Variance_{residual}}}$

```{r ICC, include=FALSE}

0.08592 / (0.08592+0.10779)

```

Roughly 44.35% of the total variance is explained by the random intercept effect ID. 

## Exploring fixed effects
We gathered a multitude of interesting variables that might influence the SoC reported by participants after every trial. In an initial model, we will simply throw all of the in the fixed effects structure and see what sticks.

```{r fix_1, include=FALSE}

fix.1 <- lmer(sqrt(SoC) ~ expectancy + N_drift*crashed + trials_since_last_crash + N_consecutive_crash_success + 
                Neuroticism + Extraversion + Openness + Conscientiousness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.1)

```

**None** of the NEO-FFI variables significantly influence the predicted variable SoC. We might only want to look out for *Neuroticism*. When deleting some fixed effects there might be more variance loading on *Neuroticism*, revealing a significant effect. Let's see...

For *trials_since_last_crash* we will put in a variable that assesses whether there was a crash in the last trial. This is less informative due to only being binary, but we may reduce complexity this way.

```{r fix_2, include=FALSE}

fix.2 <- lmer(sqrt(SoC) ~ expectancy + N_drift*crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                Neuroticism + Extraversion + Openness + Conscientiousness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.2)

```

We will keep *crashed_in_last_trial* instead of *trials_since_last_crash*. It's estimate is higher while also reaching significance.

Now we can go on and delete redundant variables, starting with *Conscientiousness* (the one with the worst p value).

```{r fix_3, include=FALSE}

fix.3 <- lmer(sqrt(SoC) ~ expectancy + N_drift*crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                Neuroticism + Extraversion + Openness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.3)

```

Like we expected, all the other dimensions absorbed the variance of the deleted *Conscientiousness*. The NEO-FFI dimensions seem to be highly correlated in their effect on SoC.

```{r 2vs3, include=FALSE}

anova(fix.2, fix.3)

```
And we can delete *Conscientiousness*. It will increase the predictive power of our model (referring to BIC, smaller is better here) while the model likelihood not being significantly different.

Let's try to reduce the model structure even more: deleting *Agreeableness*.
```{r fix_4, include=FALSE}

fix.4 <- lmer(sqrt(SoC) ~ expectancy + N_drift*crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                Neuroticism + Extraversion + Openness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.4)

anova(fix.3, fix.4)

```
Again a safe deletion without significantly affecting model likelihood but increasing predictive power.

Proceeding by deleting *Extraversion*:
```{r fix_5, include=FALSE}

fix.5 <- lmer(sqrt(SoC) ~ expectancy + N_drift*crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                Neuroticism + Openness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.5)

anova(fix.4, fix.5)

```
We will proceed with fix.5.

Deleting *Neuroticism*:
```{r fix_6, include=FALSE}

fix.6 <- lmer(sqrt(SoC) ~ expectancy + N_drift*crashed + crashed_in_last_trial + N_consecutive_crash_success + Openness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.6)

anova(fix.5, fix.6)

```
Proceeding with fix.6.

Let's see whether we can safely delete *Openness* as well:
```{r fix_7, include=FALSE}

fix.7 <- lmer(sqrt(SoC) ~ expectancy + N_drift*crashed + crashed_in_last_trial + N_consecutive_crash_success + (1|ID), data = soc_data, REML=FALSE)
summary(fix.7)

anova(fix.6, fix.7)

```
Proceeding with fix.7

Now we can actually try to simplify the model fixed effects structure even more. This will mean that we delete a significant effect, but we will try anyway:

Deleting *crashed_in_last_trial*:
```{r fix_8, include=FALSE}

fix.8 <- lmer(sqrt(SoC) ~ expectancy + N_drift*crashed + N_consecutive_crash_success + (1|ID), data = soc_data, REML=FALSE)
summary(fix.8)

anova(fix.7, fix.8)

```
Here we see a significant difference in the likelihood of the model (referring to Pr(>Chisq)). The BIC decreases, but the significance tells us to not just throw out *crashed_in_last_trial*. We will thus proceed with fix.7.

## Exploring random effects structure
Now we're going to open up the slopes of our effects left in the model to change from one ID to the next.

We're using the same approach as before: we will simply dump all possible random slope effects into the random effects structure of the model and see what sticks. We will omit the neo-ffi variables here, as none of those are left in the model.

```{r random_1, include=FALSE}

random.1 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + expectancy + N_drift*crashed + crashed_in_last_trial + N_consecutive_crash_success|ID), data = soc_data, REML=FALSE)
summary(random.1)

```
*fixed-effect model matrix is rank deficient so dropping 1 column / coefficient* is warning that we can ignore for now.
*boundary (singular) fit: see help('isSingular')* and *Warning: Model failed to converge...* however we have to take seriously. They tell us that our model is **overparameterized** (we have too many random slopes in there that do not vary across IDs). We have to kick out random slopes.

Starting with *N_consecutive_crash_success*:
```{r random_2, include=FALSE}

random.2 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + expectancy + N_drift*crashed + crashed_in_last_trial |ID), data = soc_data, REML=FALSE)
summary(random.2)

```
We still get the warnings. Let's also drop *crashed_in_last_trial*: 


```{r random_3, include=FALSE}

random.3 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + expectancy + N_drift*crashed |ID), data = soc_data, REML=FALSE)
summary(random.3)

```

Deleting N_drift from random slopes (it's the smallest effect and may be hard to split up upon IDs):
```{r random_4, include=FALSE}

random.4 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + expectancy + crashed |ID), data = soc_data, REML=FALSE)

```
Ok the model converges. We will start here: random.4 is our most complex model and we try to switch in and out random slopes to see whether other random effects structures better capture the effects the data.

Deleting crashed:
```{r random_5, include=FALSE}

random.5 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + expectancy |ID), data = soc_data, REML=FALSE)

```

Deleting expectancy instead:
```{r random_6, include=FALSE}

random.6 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + crashed |ID), data = soc_data, REML=FALSE)


```

```{r model_comparison1, echo=FALSE}

anova(fix.7, random.4, random.5, random.6)

```
random.4 has the lowest BIC, meaning that it's our best fitting model.

We will now try to add some of the other random slopes again while also keeping *expectancy* and *crashed*.

Starting with *N_drift* (this time without the interaction effect)
```{r random_7, include=FALSE}

random.7 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + expectancy + N_drift + crashed |ID), data = soc_data, REML=FALSE)
#summary(random.7)

```

Adding *crashed_in_last_trial* back in:
```{r random_8, include=FALSE}

random.8 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + expectancy + crashed + crashed_in_last_trial | ID), data = soc_data, REML=FALSE)
#summary(random.8)

```
Nope *crashed_in_last_trial* seems to cause some issues with convergence. We will omit random.8 in model selection later.

What about *N_consecutive_crash_success*?
```{r random_9, include=FALSE}

random.9 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + expectancy + crashed + N_consecutive_crash_success | ID), data = soc_data, REML=FALSE)
#summary(random.9)

```
*N_consecutive_crash_success* also leads to non-convergence.

Let's do a comparison:
```{r model_comparison1.2, include=FALSE}

anova(random.4, random.7)

```

*random.7* "wins." In a last effort we will try to drop *expectancy*:
```{r random_10, include=FALSE}

random.10 <- lmer(sqrt(SoC) ~ expectancy + N_drift * crashed + crashed_in_last_trial + N_consecutive_crash_success + 
                   (1 + N_drift + crashed |ID), data = soc_data, REML=FALSE)
#summary(random.10)

```

```{r model_comparison1.3, include=FALSE}

anova(random.7, random.10)

```
BIX increases and we see a significant difference in the model likelihood. This means we would give up explanatory power when dropping *expectancy*. random.7 therefore is our final model!

```{r taking_a_look, echo=FALSE}

summary(random.7)

```

## Generating simulations based on the final selected model

parametric bootstrap:
```{r bootstrap1, include=FALSE}

confint(random.7, nsim=N_iterations, parm=c('expectancy', 'N_drift', 'crashed', 'crashed_in_last_trial', 'N_consecutive_crash_success'), method='boot')

```
We got a few failed convergences here, but that is ok, we chose the most complex model after all.

Results:
                                   2.5 %        97.5 %
expectancy                   0.061970392  0.1172224888
N_drift                      0.051522884  0.1010306586
crashed                     -0.377887632 -0.2715667616
crashed_in_last_trial       -0.045589722 -0.0128242066
N_consecutive_crash_success -0.001689916 -0.0003068576


# Predicting Expectancy
We hypothesize that the variable *expectancy* reflects the prior belief of participants about the self-efficiency. We will explore what constitutes this prior, i.e. which variables best explain *expectancy*. We already have a candidates in mind...

## Box Cox - analysis
```{r box_cox2, include=FALSE}

lambda_expect <- boxcox(lm(soc_data$expectancy ~ 1))

lambda_expect$x[which(lambda_expect$y == max(lambda_expect$y))]

```

lambda, the expected value is close to 1.0. In this case we won't transform the predicted variable.

## ICC for randon intercept effect ID

```{r null_model_expect, include=FALSE}

null.2 <- lmer(expectancy ~ 1 + (1|ID), data = soc_data, REML=FALSE)
summary(null.2)

```

Inter-Class Correlation
```{r ICC2, include=FALSE}

1.2218 / (1.2218+0.7984)

```

ID explains roughly 60.48% of the total variance in expectancy.

## Exploring fixed effects
Same procedure as above...

```{r fix_1, include=FALSE}

fix.1 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Neuroticism + Extraversion + Openness + Conscientiousness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.1)

```

We see that the NEO-FFI dimensions do not significantly influence expectancy. Only *Openness* "approaches" significance. We will try do delete the other dimensions first. After that we might drop *crashed_in_last_trial*.

Starting with dropping *Conscientiousness*
```{r fix_2, include=FALSE}

fix.2 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Neuroticism + Extraversion + Openness + Agreeableness + (1|ID), data = soc_data, REML=FALSE)
summary(fix.2)

```

```{r 1vs2, include=FALSE}

anova(fix.1, fix.2)

```

Proceeding with fix.2

Let's see if we can reduce complexity even more. Dropping *Agreeableness*:
```{r fix_3, include=FALSE}

fix.3 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Neuroticism + Extraversion + Openness + (1|ID), data = soc_data, REML=FALSE)
#summary(fix.3)

anova(fix.2, fix.3)

```
Again deleting the effect is advised. Proceeding with fix.3.

Dropping *Neuroticism*
```{r fix_4, include=FALSE}

fix.4 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Extraversion + Openness + (1|ID), data = soc_data, REML=FALSE)
#summary(fix.4)

anova(fix.3, fix.4)

```
Dropping *Neuroticism* also decreases BIC and doesn't significantly affects model likelihood. Proceeding with fix.4.

What about *Extraversion*?
```{r fix_5, include=FALSE}

fix.5 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial + 
                Openness + (1|ID), data = soc_data, REML=FALSE)
#summary(fix.5)

anova(fix.4, fix.5)

```
Proceeding with fix.5.

Can we also drop *Openness* the last NEO-FFI dimension?
```{r fix_6, include=FALSE}

fix.6 <- lmer(expectancy ~  crashed_in_last_trial + N_consecutive_crash_success + SoC_last_trial 
              + (1|ID), data = soc_data, REML=FALSE)
#summary(fix.6)

anova(fix.5, fix.6)

```
We can safely throw out all the NEO-FFI dimensions. 

Now we will attempt to drop *crashed_in_last_trial*. We strongly assumed that this will definitely affect expectancy, but it didn't show significance earlier, so let's see:
```{r fix_7, include=FALSE}

fix.7 <- lmer(expectancy ~ N_consecutive_crash_success + SoC_last_trial 
              + (1|ID), data = soc_data, REML=FALSE)
#summary(fix.7)

anova(fix.6, fix.7)

```
*crashed_in_last_trial* is also dropped. Proceeding with fix.7.

## Exploring random effects structure

We only have 2 fixed effects left. The model is not complex, but now we will introduce random slope effects into the model and see whether either fixed effect changes across IDs.

```{r random_1, include=FALSE}

random.1 <- lmer(expectancy ~   N_consecutive_crash_success + SoC_last_trial 
                 + (1 + N_consecutive_crash_success + SoC_last_trial |ID), data = soc_data, REML=FALSE)
summary(random.1)

```

Model failed to converge... We need to reduce the complexity of the random effects structure. 

Deleting *N_consecutive_crash_success* (arbitrarily chosen starting point):
```{r random_2, include=FALSE}

random.2 <- lmer(expectancy ~ N_consecutive_crash_success + SoC_last_trial 
                 + (1 + SoC_last_trial |ID), data = soc_data, REML=FALSE)
summary(random.2)

```
Ok, this model converges. We will take it in the model selection.

Deleting *SoC_last_trial* instead:
```{r random_3, include=FALSE}

random.3 <- lmer(expectancy ~ N_consecutive_crash_success + SoC_last_trial 
                 + (1 + N_consecutive_crash_success |ID), data = soc_data, REML=FALSE)
summary(random.3)

```
Keeping *N_consecutive_crash_success* as random slope effect causes convergence issues. We'll leave it out.

```{r model_comparison2, include=FALSE}

anova(fix.7, random.2)

```
random.2 the model with the random slope effect of *SoC_last_trial* yields the lower BIC. We will use random.2 for our hypothesis testing.


```{r final_selected_model2, include=FALSE}

summary(random.2)

```

parametric bootstrap:
```{r bootstrap2, include=FALSE}

confint(random.2, nsim=N_iterations, parm=c('N_consecutive_crash_success', 'SoC_last_trial'), method='boot')

```
Results:
                                  2.5 %      97.5 %
N_consecutive_crash_success 0.004662195 0.009166225
SoC_last_trial              0.298864685 0.436390573


